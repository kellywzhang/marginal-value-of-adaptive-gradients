\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1} \href  {https://arxiv.org/pdf/1611.03530.pdf}{Understanding Deep Learning Requires Rethinking Generalization} }{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Generalization in Over-Parameterized Settings}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Descent!}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}SGD}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Momentum}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Adaptive Gradient Methods}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Newton's Method}{4}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}AdaGrad (by Duchi, Hazan, and Singer, 2011)}{5}{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}RMSProp (by Hinton, unpublished)}{5}{subsubsection.3.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}ADAM (by Kingma and Ba, 2014)}{5}{subsubsection.3.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simple Setting for Analysis: Gradient Descent for Linear Regression}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Calculating the Gradient}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Non-adaptive methods}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Adaptive methods}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Adaptivity Can Overfit}{9}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Problem Setting}{9}{subsubsection.4.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}SGD Solution}{9}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}AdaGrad Solution}{9}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Investigations}{11}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Methodology}{11}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Learning Rate Tuning}{11}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Training Objective and Models}{11}{subsubsection.5.1.2}}
\bibstyle{alpha}
\bibdata{sample}
\@writefile{toc}{\contentsline {section}{\numberline {6}Take-Aways}{13}{section.6}}
